{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract features, retrain Sherlock and generate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script below first downloads the data (roughly 700K samples), then extract features from the raw data values. <br>\n",
    "If you want to skip this step, you can follow the steps below the feature extraction to load the preprocessed data, \n",
    "retrain Sherlock and generate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score\n",
    "import tensorflow as tf\n",
    "from ast import literal_eval\n",
    "\n",
    "from sherlock import helpers\n",
    "from sherlock.features.preprocessing import extract_features, convert_string_lists_to_lists, prepare_feature_extraction\n",
    "from sherlock.deploy.train_sherlock import train_sherlock\n",
    "from sherlock.deploy.predict_sherlock import predict_sherlock\n",
    "\n",
    "from pympler import muppy, summary\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data\n",
    "This will download the raw values and preprocessed files, the corresponding labels as well as a few other supporting files:\n",
    "- `download_data()` will download 3.6GB of data into the `data/` directory.\n",
    "- `prepare_feature_extraction()` will download +/- 800 MB of data into the `features/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading the raw and preprocessed data into ../data/data.zip.\n",
      "Data was downloaded.\n",
      "Preparing feature extraction by downloading 2 files:\n",
      "        \n",
      " ../sherlock/features/glove.6B.50d.txt and \n",
      " ../sherlock/features/par_vec_trained_400.pkl.docvecs.vectors_docs.npy.\n",
      "        \n",
      "All files for extracting word and paragraph embeddings are present.\n"
     ]
    }
   ],
   "source": [
    "helpers.download_data()\n",
    "prepare_feature_extraction()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in raw data\n",
    "You can skip this step if you want to use a preprocessed data file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report memory usage (can be slow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_memory = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features\n",
    "It is important that the string-representations of lists are first converted into lists of strings.\n",
    "The labels should be a list of semantic types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "X_test_filename = f'../data/work/test_{timestr}.csv'\n",
    "X_train_filename = f'../data/work/train_{timestr}.csv'\n",
    "X_validation_filename = f'../data/work/validation_{timestr}.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "from functional import pseq\n",
    "from sherlock.functional import as_py_str, to_literal, randomise_sample, as_str_series, dropna, extract_features, normalise_string_whitespace\n",
    "from pyarrow.parquet import ParquetFile\n",
    "\n",
    "\n",
    "def load_parquet_values(path):\n",
    "    pf = ParquetFile(source = path)\n",
    "    row_df = pf.read_row_group(0)\n",
    "    \n",
    "    return row_df['values']\n",
    "\n",
    "def extract_features_to_csv(output_path, parquet_values):\n",
    "    verify_keys = False\n",
    "    first_keys = None\n",
    "    i = 0\n",
    "\n",
    "    start = datetime.now()\n",
    "\n",
    "    print(f'Starting {output_path} at {start}')\n",
    "\n",
    "    with open(output_path, \"w\") as outfile:\n",
    "        csvwriter = csv.writer(outfile)\n",
    "\n",
    "        # Comparable performance with using pool.imap directly, but the code is *much* cleaner\n",
    "        for features in pseq(parquet_values, processes=6, partition_size=10)\\\n",
    "            .map(as_py_str)\\\n",
    "            .map(to_literal)\\\n",
    "            .map(randomise_sample)\\\n",
    "            .map(normalise_string_whitespace)\\\n",
    "            .map(as_str_series)\\\n",
    "            .map(dropna)\\\n",
    "            .map(extract_features):\n",
    "                i = i+1\n",
    "\n",
    "                keys=features.keys()\n",
    "\n",
    "                if first_keys is None:\n",
    "                    first_keys = keys\n",
    "                    first_keys_str = ','.join(keys)\n",
    "\n",
    "                    print(f'Exporting {len(first_keys)} column features')\n",
    "\n",
    "                    csvwriter.writerow(first_keys)\n",
    "                elif verify_keys:\n",
    "                    keys_str = ','.join(keys)\n",
    "                    if first_keys_str != keys_str:\n",
    "                        key_list = list(keys)\n",
    "\n",
    "                        print(f'keys are NOT equal. k1 len={len(first_keys)}, k2 len={len(keys)}')\n",
    "\n",
    "                        for idx, k1 in enumerate(first_keys):\n",
    "                            k2 = key_list[idx]\n",
    "\n",
    "                            if k1 != k2:\n",
    "                                print(f'{k1} != {k2}')\n",
    "\n",
    "                csvwriter.writerow(list(features.values()))\n",
    "\n",
    "    print(f'Finished. Processed {i} rows in {datetime.now() - start}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXTRACT FEATURES TO CSV (NEW METHOD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing feature extraction by downloading 2 files:\n",
      "        \n",
      " ../sherlock/features/glove.6B.50d.txt and \n",
      " ../sherlock/features/par_vec_trained_400.pkl.docvecs.vectors_docs.npy.\n",
      "        \n",
      "All files for extracting word and paragraph embeddings are present.\n",
      "Initialising word embeddings\n",
      "Initialise Word Embeddings process took 0:00:05.790520 seconds.\n",
      "Initialise Doc2Vec Model, 400 dim, process took 0:00:01.180743 seconds. (filename = ../sherlock/features/par_vec_trained_400.pkl)\n"
     ]
    }
   ],
   "source": [
    "# ensure embedding initialisation is outside of timing for extract_features\n",
    "from sherlock.features.word_embeddings import initialise_word_embeddings\n",
    "from sherlock.features.paragraph_vectors import initialise_pretrained_model\n",
    "from sherlock.features.preprocessing import prepare_feature_extraction\n",
    "\n",
    "prepare_feature_extraction()\n",
    "initialise_word_embeddings()\n",
    "initialise_pretrained_model(400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting ../data/work/test_20201226-184943.csv at 2020-12-26 18:51:24.146382\n",
      "Exporting 1578 column features\n",
      "Finished. Processed 137353 rows in 0:28:58.703536\n"
     ]
    }
   ],
   "source": [
    "values = load_parquet_values(\"../data/raw/test_values.parquet\")\n",
    "\n",
    "extract_features_to_csv(X_test_filename, values)\n",
    "\n",
    "values = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished at 2020-12-26 19:20:22.960766\n"
     ]
    }
   ],
   "source": [
    "print(f'Finished at {datetime.now()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAIN SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting ../data/work/train_20201226-184943.csv at 2020-12-26 19:20:25.240876\n",
      "Exporting 1578 column features\n",
      "Finished. Processed 412059 rows in 1:18:38.726698\n"
     ]
    }
   ],
   "source": [
    "values = load_parquet_values(\"../data/raw/train_values.parquet\")\n",
    "\n",
    "extract_features_to_csv(X_train_filename, values)\n",
    "\n",
    "values = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished at 2020-12-26 20:39:04.195718\n"
     ]
    }
   ],
   "source": [
    "print(f'Finished at {datetime.now()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VALIDATION SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting ../data/work/validation_20201226-184943.csv at 2020-12-26 20:39:05.082241\n",
      "Exporting 1578 column features\n",
      "Finished. Processed 137353 rows in 0:26:10.045277\n"
     ]
    }
   ],
   "source": [
    "values = load_parquet_values(\"../data/raw/val_values.parquet\")\n",
    "\n",
    "extract_features_to_csv(X_validation_filename, values)\n",
    "\n",
    "values = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished at 2020-12-26 21:05:15.229655\n"
     ]
    }
   ],
   "source": [
    "print(f'Finished at {datetime.now()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXTRACT FEATURES TO CSV (**OLD** METHOD) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST SET (OLD METHOD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if report_memory:\n",
    "    all_objects = muppy.get_objects()\n",
    "    sum1 = summary.summarize(all_objects)\n",
    "    # Prints out a summary of the large objects\n",
    "    summary.print_(sum1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples = pd.read_parquet('../data/raw/test_values.parquet')\n",
    "test_labels = pd.read_parquet('../data/raw/test_labels.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 8 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "test_samples_converted, y_test = convert_string_lists_to_lists(test_samples, test_labels, \"values\", \"type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# free memory\n",
    "test_samples = None\n",
    "test_labels = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20368     [Central Missouri, unattached, unattached, Kan...\n",
       "664102    [95, 100, 95, 89, 84, 91, 88, 94, 75, 78, 90, ...\n",
       "366813    [Katie Crews, Christian Hiraldo, Alex Estrada,...\n",
       "530567    [Christian, Non-Christian, Unreported, Jewish,...\n",
       "176253    [AAF-McQuay Canada Inc., AAF-McQuay Canada Inc...\n",
       "Name: values, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_samples_converted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['affiliation', 'weight', 'jockey', 'religion', 'company']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output \"head\" \n",
    "y_test[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Given that feature extraction can take long, we only take the first 100 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_subset = y_test[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialising word embeddings\n",
      "Initialise Word Embeddings process took 0:00:05.994311 seconds.\n",
      "Initialise Doc2Vec Model, 400 dim, process took 0:00:01.203899 seconds.\n"
     ]
    }
   ],
   "source": [
    "# ensure embedding initialisation is outside of timing for extract_features\n",
    "from sherlock.features.word_embeddings import initialise_word_embeddings\n",
    "from sherlock.features.paragraph_vectors import initialise_pretrained_model\n",
    "\n",
    "initialise_word_embeddings()\n",
    "initialise_pretrained_model(400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if report_memory:\n",
    "    all_objects = muppy.get_objects()\n",
    "    sum1 = summary.summarize(all_objects)\n",
    "    # Prints out a summary of the large objects\n",
    "    summary.print_(sum1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features:   7%|▋         | 7/100 [00:00<00:02, 31.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting 1578 column features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting Features: 100%|██████████| 100/100 [00:02<00:00, 40.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract Features (test) process took 0:00:02.489302 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "#X_test = extract_features(test_samples_converted.head(n=100))\n",
    "extract_features(X_test_filename, test_samples_converted.head(n=100))\n",
    "\n",
    "print(f'Extract Features (test) process took {datetime.now() - start} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples_converted = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if report_memory:\n",
    "    all_objects = muppy.get_objects()\n",
    "    sum1 = summary.summarize(all_objects)\n",
    "    # Prints out a summary of the large objects\n",
    "    summary.print_(sum1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# over all, without memory management\n",
    "# Extract Features (test) process took 3:40:25.799880 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline\n",
    "# Extract Features (test) process took 0:11:04.137081 seconds.\n",
    "\n",
    "# Iterations\n",
    "# Extract Features (test) process took 0:00:56.671353 seconds. (cache word embeddings)\n",
    "# Extract Features (test) process took 0:00:13.523261 seconds. (cache Doc2Vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAIN SET (OLD METHOD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = pd.read_parquet('../data/raw/train_values.parquet')\n",
    "train_labels = pd.read_parquet('../data/raw/train_labels.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples_converted, y_train = convert_string_lists_to_lists(train_samples, train_labels, \"values\", \"type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# free memory\n",
    "train_samples = None\n",
    "train_labels = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_subset = y_train[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "extract_features(X_train_filename, train_samples_converted)\n",
    "\n",
    "print(f'Extract Features (train) process took {datetime.now() - start} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples_converted = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VALIDATION SET (OLD METHOD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_samples = pd.read_parquet('../data/raw/val_values.parquet')\n",
    "validation_labels = pd.read_parquet('../data/raw/val_labels.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_samples_converted, y_validation = convert_string_lists_to_lists(validation_samples, validation_labels, \"values\", \"type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# free memory\n",
    "validation_samples = None\n",
    "validation_labels = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_validation_subset = y_validation[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "extract_features(X_validation_filename, validation_samples_converted)\n",
    "\n",
    "print(f'Extract Features (validation) process took {datetime.now() - start} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_samples_converted = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Locally Processed Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestr = '20201224-105345'\n",
    "\n",
    "X_test_filename = f'../data/work/test_{timestr}.csv'\n",
    "X_train_filename = f'../data/work/train_{timestr}.csv'\n",
    "X_validation_filename = f'../data/work/validation_{timestr}.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Features (test) process took 0:00:37.855500 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "X_test = pd.read_csv(X_test_filename)\n",
    "\n",
    "print(f'Load Features (test) process took {datetime.now() - start} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_[0]-agg-any</th>\n",
       "      <th>n_[0]-agg-all</th>\n",
       "      <th>n_[0]-agg-mean</th>\n",
       "      <th>n_[0]-agg-var</th>\n",
       "      <th>n_[0]-agg-min</th>\n",
       "      <th>n_[0]-agg-max</th>\n",
       "      <th>n_[0]-agg-median</th>\n",
       "      <th>n_[0]-agg-sum</th>\n",
       "      <th>n_[0]-agg-kurtosis</th>\n",
       "      <th>n_[0]-agg-skewness</th>\n",
       "      <th>...</th>\n",
       "      <th>par_vec_390</th>\n",
       "      <th>par_vec_391</th>\n",
       "      <th>par_vec_392</th>\n",
       "      <th>par_vec_393</th>\n",
       "      <th>par_vec_394</th>\n",
       "      <th>par_vec_395</th>\n",
       "      <th>par_vec_396</th>\n",
       "      <th>par_vec_397</th>\n",
       "      <th>par_vec_398</th>\n",
       "      <th>par_vec_399</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001000</td>\n",
       "      <td>0.001006</td>\n",
       "      <td>-0.000732</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>-0.001023</td>\n",
       "      <td>0.000647</td>\n",
       "      <td>0.000841</td>\n",
       "      <td>0.000608</td>\n",
       "      <td>0.000641</td>\n",
       "      <td>0.000092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.166205</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.016667</td>\n",
       "      <td>1.420094</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000429</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>-0.000498</td>\n",
       "      <td>-0.001104</td>\n",
       "      <td>0.000380</td>\n",
       "      <td>-0.000146</td>\n",
       "      <td>0.000720</td>\n",
       "      <td>-0.001103</td>\n",
       "      <td>-0.000719</td>\n",
       "      <td>-0.001237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001026</td>\n",
       "      <td>-0.000066</td>\n",
       "      <td>-0.000258</td>\n",
       "      <td>-0.000787</td>\n",
       "      <td>0.001085</td>\n",
       "      <td>-0.000515</td>\n",
       "      <td>-0.000086</td>\n",
       "      <td>-0.000965</td>\n",
       "      <td>-0.001070</td>\n",
       "      <td>-0.000936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001046</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>-0.000720</td>\n",
       "      <td>0.000739</td>\n",
       "      <td>-0.000891</td>\n",
       "      <td>-0.000133</td>\n",
       "      <td>-0.001018</td>\n",
       "      <td>-0.000630</td>\n",
       "      <td>-0.000442</td>\n",
       "      <td>-0.000617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.020080</td>\n",
       "      <td>0.035741</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>96.521561</td>\n",
       "      <td>9.784149</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000337</td>\n",
       "      <td>-0.000999</td>\n",
       "      <td>-0.000844</td>\n",
       "      <td>0.000755</td>\n",
       "      <td>-0.001139</td>\n",
       "      <td>0.000474</td>\n",
       "      <td>-0.000506</td>\n",
       "      <td>0.000785</td>\n",
       "      <td>0.000571</td>\n",
       "      <td>-0.000209</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1578 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_[0]-agg-any  n_[0]-agg-all  n_[0]-agg-mean  n_[0]-agg-var  n_[0]-agg-min  \\\n",
       "0          False          False        0.000000       0.000000              0   \n",
       "1           True          False        0.210526       0.166205              0   \n",
       "2          False          False        0.000000       0.000000              0   \n",
       "3          False          False        0.000000       0.000000              0   \n",
       "4           True          False        0.020080       0.035741              0   \n",
       "\n",
       "   n_[0]-agg-max  n_[0]-agg-median  n_[0]-agg-sum  n_[0]-agg-kurtosis  \\\n",
       "0              0               0.0              0           -3.000000   \n",
       "1              1               0.0              4            0.016667   \n",
       "2              0               0.0              0           -3.000000   \n",
       "3              0               0.0              0           -3.000000   \n",
       "4              2               0.0              5           96.521561   \n",
       "\n",
       "   n_[0]-agg-skewness  ...  par_vec_390  par_vec_391  par_vec_392  \\\n",
       "0            0.000000  ...    -0.001000     0.001006    -0.000732   \n",
       "1            1.420094  ...    -0.000429     0.000148    -0.000498   \n",
       "2            0.000000  ...    -0.001026    -0.000066    -0.000258   \n",
       "3            0.000000  ...     0.001046     0.000031    -0.000720   \n",
       "4            9.784149  ...     0.000337    -0.000999    -0.000844   \n",
       "\n",
       "   par_vec_393  par_vec_394  par_vec_395  par_vec_396  par_vec_397  \\\n",
       "0     0.000139    -0.001023     0.000647     0.000841     0.000608   \n",
       "1    -0.001104     0.000380    -0.000146     0.000720    -0.001103   \n",
       "2    -0.000787     0.001085    -0.000515    -0.000086    -0.000965   \n",
       "3     0.000739    -0.000891    -0.000133    -0.001018    -0.000630   \n",
       "4     0.000755    -0.001139     0.000474    -0.000506     0.000785   \n",
       "\n",
       "   par_vec_398  par_vec_399  \n",
       "0     0.000641     0.000092  \n",
       "1    -0.000719    -0.001237  \n",
       "2    -0.001070    -0.000936  \n",
       "3    -0.000442    -0.000617  \n",
       "4     0.000571    -0.000209  \n",
       "\n",
       "[5 rows x 1578 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Features (train) process took 0:02:02.434049 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "X_train = pd.read_csv(X_train_filename)\n",
    "\n",
    "print(f'Load Features (train) process took {datetime.now() - start} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_[0]-agg-any</th>\n",
       "      <th>n_[0]-agg-all</th>\n",
       "      <th>n_[0]-agg-mean</th>\n",
       "      <th>n_[0]-agg-var</th>\n",
       "      <th>n_[0]-agg-min</th>\n",
       "      <th>n_[0]-agg-max</th>\n",
       "      <th>n_[0]-agg-median</th>\n",
       "      <th>n_[0]-agg-sum</th>\n",
       "      <th>n_[0]-agg-kurtosis</th>\n",
       "      <th>n_[0]-agg-skewness</th>\n",
       "      <th>...</th>\n",
       "      <th>par_vec_390</th>\n",
       "      <th>par_vec_391</th>\n",
       "      <th>par_vec_392</th>\n",
       "      <th>par_vec_393</th>\n",
       "      <th>par_vec_394</th>\n",
       "      <th>par_vec_395</th>\n",
       "      <th>par_vec_396</th>\n",
       "      <th>par_vec_397</th>\n",
       "      <th>par_vec_398</th>\n",
       "      <th>par_vec_399</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000590</td>\n",
       "      <td>-0.000621</td>\n",
       "      <td>-0.000125</td>\n",
       "      <td>-0.000316</td>\n",
       "      <td>0.000560</td>\n",
       "      <td>-0.000397</td>\n",
       "      <td>0.000358</td>\n",
       "      <td>0.000512</td>\n",
       "      <td>0.000716</td>\n",
       "      <td>-0.000111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000542</td>\n",
       "      <td>-0.000094</td>\n",
       "      <td>0.001015</td>\n",
       "      <td>0.000624</td>\n",
       "      <td>-0.000116</td>\n",
       "      <td>0.000525</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>-0.000882</td>\n",
       "      <td>0.000631</td>\n",
       "      <td>-0.000760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001092</td>\n",
       "      <td>-0.000314</td>\n",
       "      <td>-0.000646</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000335</td>\n",
       "      <td>0.001145</td>\n",
       "      <td>0.000908</td>\n",
       "      <td>-0.000703</td>\n",
       "      <td>0.000594</td>\n",
       "      <td>-0.000548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000389</td>\n",
       "      <td>-0.001117</td>\n",
       "      <td>0.001101</td>\n",
       "      <td>0.001177</td>\n",
       "      <td>-0.000984</td>\n",
       "      <td>0.000332</td>\n",
       "      <td>-0.000916</td>\n",
       "      <td>0.000818</td>\n",
       "      <td>0.000485</td>\n",
       "      <td>-0.000822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000048</td>\n",
       "      <td>-0.000403</td>\n",
       "      <td>-0.000994</td>\n",
       "      <td>0.000605</td>\n",
       "      <td>-0.000721</td>\n",
       "      <td>0.000525</td>\n",
       "      <td>0.000824</td>\n",
       "      <td>-0.001220</td>\n",
       "      <td>-0.000475</td>\n",
       "      <td>-0.000648</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1578 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_[0]-agg-any  n_[0]-agg-all  n_[0]-agg-mean  n_[0]-agg-var  n_[0]-agg-min  \\\n",
       "0          False          False             0.0            0.0              0   \n",
       "1          False          False             0.0            0.0              0   \n",
       "2          False          False             0.0            0.0              0   \n",
       "3          False          False             0.0            0.0              0   \n",
       "4          False          False             0.0            0.0              0   \n",
       "\n",
       "   n_[0]-agg-max  n_[0]-agg-median  n_[0]-agg-sum  n_[0]-agg-kurtosis  \\\n",
       "0              0               0.0              0                -3.0   \n",
       "1              0               0.0              0                -3.0   \n",
       "2              0               0.0              0                -3.0   \n",
       "3              0               0.0              0                -3.0   \n",
       "4              0               0.0              0                -3.0   \n",
       "\n",
       "   n_[0]-agg-skewness  ...  par_vec_390  par_vec_391  par_vec_392  \\\n",
       "0                 0.0  ...     0.000590    -0.000621    -0.000125   \n",
       "1                 0.0  ...    -0.000542    -0.000094     0.001015   \n",
       "2                 0.0  ...     0.001092    -0.000314    -0.000646   \n",
       "3                 0.0  ...    -0.000389    -0.001117     0.001101   \n",
       "4                 0.0  ...    -0.000048    -0.000403    -0.000994   \n",
       "\n",
       "   par_vec_393  par_vec_394  par_vec_395  par_vec_396  par_vec_397  \\\n",
       "0    -0.000316     0.000560    -0.000397     0.000358     0.000512   \n",
       "1     0.000624    -0.000116     0.000525     0.000014    -0.000882   \n",
       "2     0.000006     0.000335     0.001145     0.000908    -0.000703   \n",
       "3     0.001177    -0.000984     0.000332    -0.000916     0.000818   \n",
       "4     0.000605    -0.000721     0.000525     0.000824    -0.001220   \n",
       "\n",
       "   par_vec_398  par_vec_399  \n",
       "0     0.000716    -0.000111  \n",
       "1     0.000631    -0.000760  \n",
       "2     0.000594    -0.000548  \n",
       "3     0.000485    -0.000822  \n",
       "4    -0.000475    -0.000648  \n",
       "\n",
       "[5 rows x 1578 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Features (validation) process took 0:00:38.160077 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "X_validation = pd.read_csv(X_validation_filename)\n",
    "\n",
    "print(f'Load Features (validation) process took {datetime.now() - start} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_[0]-agg-any</th>\n",
       "      <th>n_[0]-agg-all</th>\n",
       "      <th>n_[0]-agg-mean</th>\n",
       "      <th>n_[0]-agg-var</th>\n",
       "      <th>n_[0]-agg-min</th>\n",
       "      <th>n_[0]-agg-max</th>\n",
       "      <th>n_[0]-agg-median</th>\n",
       "      <th>n_[0]-agg-sum</th>\n",
       "      <th>n_[0]-agg-kurtosis</th>\n",
       "      <th>n_[0]-agg-skewness</th>\n",
       "      <th>...</th>\n",
       "      <th>par_vec_390</th>\n",
       "      <th>par_vec_391</th>\n",
       "      <th>par_vec_392</th>\n",
       "      <th>par_vec_393</th>\n",
       "      <th>par_vec_394</th>\n",
       "      <th>par_vec_395</th>\n",
       "      <th>par_vec_396</th>\n",
       "      <th>par_vec_397</th>\n",
       "      <th>par_vec_398</th>\n",
       "      <th>par_vec_399</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000717</td>\n",
       "      <td>-0.000130</td>\n",
       "      <td>0.001186</td>\n",
       "      <td>-0.000725</td>\n",
       "      <td>0.000864</td>\n",
       "      <td>-0.000655</td>\n",
       "      <td>0.001048</td>\n",
       "      <td>-0.000066</td>\n",
       "      <td>0.000996</td>\n",
       "      <td>0.000543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000436</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>0.001134</td>\n",
       "      <td>0.001093</td>\n",
       "      <td>0.000516</td>\n",
       "      <td>0.000571</td>\n",
       "      <td>0.000288</td>\n",
       "      <td>-0.001190</td>\n",
       "      <td>0.001071</td>\n",
       "      <td>0.000506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.1875</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10</td>\n",
       "      <td>-0.666667</td>\n",
       "      <td>1.154701</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000832</td>\n",
       "      <td>0.000847</td>\n",
       "      <td>0.001088</td>\n",
       "      <td>0.000945</td>\n",
       "      <td>0.000846</td>\n",
       "      <td>0.000262</td>\n",
       "      <td>0.000769</td>\n",
       "      <td>-0.000741</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>-0.000985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000233</td>\n",
       "      <td>-0.000331</td>\n",
       "      <td>-0.000233</td>\n",
       "      <td>0.000242</td>\n",
       "      <td>-0.000856</td>\n",
       "      <td>-0.001208</td>\n",
       "      <td>-0.001075</td>\n",
       "      <td>-0.000803</td>\n",
       "      <td>-0.000702</td>\n",
       "      <td>-0.000797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000578</td>\n",
       "      <td>0.000620</td>\n",
       "      <td>0.000950</td>\n",
       "      <td>0.000361</td>\n",
       "      <td>-0.000398</td>\n",
       "      <td>0.001148</td>\n",
       "      <td>-0.000720</td>\n",
       "      <td>-0.000684</td>\n",
       "      <td>0.000582</td>\n",
       "      <td>-0.000153</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1578 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_[0]-agg-any  n_[0]-agg-all  n_[0]-agg-mean  n_[0]-agg-var  n_[0]-agg-min  \\\n",
       "0          False          False            0.00         0.0000              0   \n",
       "1          False          False            0.00         0.0000              0   \n",
       "2           True          False            0.25         0.1875              0   \n",
       "3          False          False            0.00         0.0000              0   \n",
       "4          False          False            0.00         0.0000              0   \n",
       "\n",
       "   n_[0]-agg-max  n_[0]-agg-median  n_[0]-agg-sum  n_[0]-agg-kurtosis  \\\n",
       "0              0               0.0              0           -3.000000   \n",
       "1              0               0.0              0           -3.000000   \n",
       "2              1               0.0             10           -0.666667   \n",
       "3              0               0.0              0           -3.000000   \n",
       "4              0               0.0              0           -3.000000   \n",
       "\n",
       "   n_[0]-agg-skewness  ...  par_vec_390  par_vec_391  par_vec_392  \\\n",
       "0            0.000000  ...    -0.000717    -0.000130     0.001186   \n",
       "1            0.000000  ...     0.000436    -0.000001     0.001134   \n",
       "2            1.154701  ...    -0.000832     0.000847     0.001088   \n",
       "3            0.000000  ...    -0.000233    -0.000331    -0.000233   \n",
       "4            0.000000  ...     0.000578     0.000620     0.000950   \n",
       "\n",
       "   par_vec_393  par_vec_394  par_vec_395  par_vec_396  par_vec_397  \\\n",
       "0    -0.000725     0.000864    -0.000655     0.001048    -0.000066   \n",
       "1     0.001093     0.000516     0.000571     0.000288    -0.001190   \n",
       "2     0.000945     0.000846     0.000262     0.000769    -0.000741   \n",
       "3     0.000242    -0.000856    -0.001208    -0.001075    -0.000803   \n",
       "4     0.000361    -0.000398     0.001148    -0.000720    -0.000684   \n",
       "\n",
       "   par_vec_398  par_vec_399  \n",
       "0     0.000996     0.000543  \n",
       "1     0.001071     0.000506  \n",
       "2     0.000132    -0.000985  \n",
       "3    -0.000702    -0.000797  \n",
       "4     0.000582    -0.000153  \n",
       "\n",
       "[5 rows x 1578 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_validation.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impute NaN values with feature means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transpose process took 0:12:28.458519 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "train_columns_means = pd.DataFrame(X_train.mean()).transpose()\n",
    "\n",
    "print(f'Transpose process took {datetime.now() - start} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FillNA process took 0:00:27.155404 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "X_train.fillna(train_columns_means.iloc[0], inplace=True)\n",
    "X_validation.fillna(train_columns_means.iloc[0], inplace=True)\n",
    "X_test.fillna(train_columns_means.iloc[0], inplace=True)\n",
    "\n",
    "train_columns_means=None\n",
    "\n",
    "print(f'FillNA process took {datetime.now() - start} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv('train.csv')\n",
    "X_validation.to_csv('validation.csv')\n",
    "X_test.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restore Pickled Session ready for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('train.csv')\n",
    "X_validation = pd.read_csv('validation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pd.read_parquet('../data/raw/train_labels.parquet').values.flatten()\n",
    "y_validation = pd.read_parquet('../data/raw/val_labels.parquet').values.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['area', 'collection', 'team Name', ..., 'description', 'depth',\n",
       "       'product'], dtype=object)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain sherlock\n",
    "The model can be retrained using the code below. The model is currently restricted to be trained on 78 classes, the code of the model architecture will soon be added for adjusting this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started at 2020-12-26 21:45:05.876629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1226 21:45:06.117853 4443131328 deprecation.py:506] From /Users/lowecg/source/private-github/sherlock-project/venv/lib/python3.7/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W1226 21:45:06.122068 4443131328 deprecation.py:506] From /Users/lowecg/source/private-github/sherlock-project/venv/lib/python3.7/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W1226 21:45:06.129063 4443131328 deprecation.py:506] From /Users/lowecg/source/private-github/sherlock-project/venv/lib/python3.7/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W1226 21:45:06.162975 4443131328 deprecation.py:506] From /Users/lowecg/source/private-github/sherlock-project/venv/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded and compiled model, now fitting model on data.\n",
      "Train on 412059 samples, validate on 137353 samples\n",
      "Epoch 1/100\n",
      "412059/412059 [==============================] - 328s 797us/sample - loss: 2.1733 - categorical_accuracy: 0.5581 - val_loss: 1.4509 - val_categorical_accuracy: 0.7187\n",
      "Epoch 2/100\n",
      "412059/412059 [==============================] - 277s 671us/sample - loss: 1.5015 - categorical_accuracy: 0.7007 - val_loss: 1.2831 - val_categorical_accuracy: 0.7531\n",
      "Epoch 3/100\n",
      "412059/412059 [==============================] - 275s 667us/sample - loss: 1.3486 - categorical_accuracy: 0.7326 - val_loss: 1.1854 - val_categorical_accuracy: 0.7728\n",
      "Epoch 4/100\n",
      "412059/412059 [==============================] - 272s 661us/sample - loss: 1.2539 - categorical_accuracy: 0.7512 - val_loss: 1.1221 - val_categorical_accuracy: 0.7845\n",
      "Epoch 5/100\n",
      "412059/412059 [==============================] - 274s 666us/sample - loss: 1.1857 - categorical_accuracy: 0.7647 - val_loss: 1.0797 - val_categorical_accuracy: 0.7921\n",
      "Epoch 6/100\n",
      "412059/412059 [==============================] - 278s 675us/sample - loss: 1.1287 - categorical_accuracy: 0.7750 - val_loss: 1.0416 - val_categorical_accuracy: 0.8001\n",
      "Epoch 7/100\n",
      "412059/412059 [==============================] - 274s 664us/sample - loss: 1.0835 - categorical_accuracy: 0.7829 - val_loss: 1.0153 - val_categorical_accuracy: 0.8048\n",
      "Epoch 8/100\n",
      "412059/412059 [==============================] - 277s 673us/sample - loss: 1.0442 - categorical_accuracy: 0.7902 - val_loss: 0.9899 - val_categorical_accuracy: 0.8123\n",
      "Epoch 9/100\n",
      "412059/412059 [==============================] - 274s 666us/sample - loss: 1.0113 - categorical_accuracy: 0.7957 - val_loss: 0.9711 - val_categorical_accuracy: 0.8143\n",
      "Epoch 10/100\n",
      "412059/412059 [==============================] - 278s 674us/sample - loss: 0.9839 - categorical_accuracy: 0.7997 - val_loss: 0.9368 - val_categorical_accuracy: 0.8183\n",
      "Epoch 11/100\n",
      "412059/412059 [==============================] - 279s 678us/sample - loss: 0.9562 - categorical_accuracy: 0.8043 - val_loss: 0.9182 - val_categorical_accuracy: 0.8227\n",
      "Epoch 12/100\n",
      "412059/412059 [==============================] - 280s 681us/sample - loss: 0.9326 - categorical_accuracy: 0.8085 - val_loss: 0.9003 - val_categorical_accuracy: 0.8239\n",
      "Epoch 13/100\n",
      "412059/412059 [==============================] - 282s 685us/sample - loss: 0.9108 - categorical_accuracy: 0.8122 - val_loss: 0.8861 - val_categorical_accuracy: 0.8272\n",
      "Epoch 14/100\n",
      "412059/412059 [==============================] - 276s 671us/sample - loss: 0.8912 - categorical_accuracy: 0.8157 - val_loss: 0.8719 - val_categorical_accuracy: 0.8280\n",
      "Epoch 15/100\n",
      "412059/412059 [==============================] - 278s 675us/sample - loss: 0.8737 - categorical_accuracy: 0.8181 - val_loss: 0.8698 - val_categorical_accuracy: 0.8313\n",
      "Epoch 16/100\n",
      "412059/412059 [==============================] - 277s 672us/sample - loss: 0.8565 - categorical_accuracy: 0.8209 - val_loss: 0.8440 - val_categorical_accuracy: 0.8334\n",
      "Epoch 17/100\n",
      "412059/412059 [==============================] - 278s 675us/sample - loss: 0.8412 - categorical_accuracy: 0.8234 - val_loss: 0.8314 - val_categorical_accuracy: 0.8339\n",
      "Epoch 18/100\n",
      "412059/412059 [==============================] - 276s 670us/sample - loss: 0.8269 - categorical_accuracy: 0.8264 - val_loss: 0.8393 - val_categorical_accuracy: 0.8360\n",
      "Epoch 19/100\n",
      "412059/412059 [==============================] - 277s 672us/sample - loss: 0.8153 - categorical_accuracy: 0.8272 - val_loss: 0.8209 - val_categorical_accuracy: 0.8375\n",
      "Epoch 20/100\n",
      "412059/412059 [==============================] - 274s 664us/sample - loss: 0.8025 - categorical_accuracy: 0.8297 - val_loss: 0.8107 - val_categorical_accuracy: 0.8383\n",
      "Epoch 21/100\n",
      "412059/412059 [==============================] - 279s 677us/sample - loss: 0.7917 - categorical_accuracy: 0.8315 - val_loss: 0.7938 - val_categorical_accuracy: 0.8404\n",
      "Epoch 22/100\n",
      "412059/412059 [==============================] - 277s 673us/sample - loss: 0.7815 - categorical_accuracy: 0.8336 - val_loss: 0.7950 - val_categorical_accuracy: 0.8414\n",
      "Epoch 23/100\n",
      "412059/412059 [==============================] - 279s 678us/sample - loss: 0.7707 - categorical_accuracy: 0.8352 - val_loss: 0.7863 - val_categorical_accuracy: 0.8427\n",
      "Epoch 24/100\n",
      "412059/412059 [==============================] - 283s 686us/sample - loss: 0.7630 - categorical_accuracy: 0.8363 - val_loss: 0.7910 - val_categorical_accuracy: 0.8435\n",
      "Epoch 25/100\n",
      "412059/412059 [==============================] - 278s 674us/sample - loss: 0.7528 - categorical_accuracy: 0.8384 - val_loss: 0.7740 - val_categorical_accuracy: 0.8434\n",
      "Epoch 26/100\n",
      "412059/412059 [==============================] - 281s 682us/sample - loss: 0.7446 - categorical_accuracy: 0.8398 - val_loss: 0.7719 - val_categorical_accuracy: 0.8439\n",
      "Epoch 27/100\n",
      "412059/412059 [==============================] - 280s 680us/sample - loss: 0.7358 - categorical_accuracy: 0.8413 - val_loss: 0.7634 - val_categorical_accuracy: 0.8462\n",
      "Epoch 28/100\n",
      "412059/412059 [==============================] - 278s 675us/sample - loss: 0.7296 - categorical_accuracy: 0.8423 - val_loss: 0.7556 - val_categorical_accuracy: 0.8460\n",
      "Epoch 29/100\n",
      "412059/412059 [==============================] - 275s 667us/sample - loss: 0.7221 - categorical_accuracy: 0.8436 - val_loss: 0.7467 - val_categorical_accuracy: 0.8465\n",
      "Epoch 30/100\n",
      "412059/412059 [==============================] - 279s 677us/sample - loss: 0.7147 - categorical_accuracy: 0.8447 - val_loss: 0.7705 - val_categorical_accuracy: 0.8478\n",
      "Epoch 31/100\n",
      "412059/412059 [==============================] - 279s 678us/sample - loss: 0.7087 - categorical_accuracy: 0.8460 - val_loss: 0.7443 - val_categorical_accuracy: 0.8473\n",
      "Epoch 32/100\n",
      "412059/412059 [==============================] - 278s 676us/sample - loss: 0.7054 - categorical_accuracy: 0.8466 - val_loss: 0.7462 - val_categorical_accuracy: 0.8489\n",
      "Epoch 33/100\n",
      "412059/412059 [==============================] - 280s 681us/sample - loss: 0.6986 - categorical_accuracy: 0.8477 - val_loss: 0.7502 - val_categorical_accuracy: 0.8505\n",
      "Epoch 34/100\n",
      "412059/412059 [==============================] - 282s 684us/sample - loss: 0.6932 - categorical_accuracy: 0.8484 - val_loss: 0.7389 - val_categorical_accuracy: 0.8500\n",
      "Epoch 35/100\n",
      "412059/412059 [==============================] - 279s 678us/sample - loss: 0.6886 - categorical_accuracy: 0.8496 - val_loss: 0.7300 - val_categorical_accuracy: 0.8512\n",
      "Epoch 36/100\n",
      "412059/412059 [==============================] - 279s 676us/sample - loss: 0.6813 - categorical_accuracy: 0.8506 - val_loss: 0.7432 - val_categorical_accuracy: 0.8509\n",
      "Epoch 37/100\n",
      "412059/412059 [==============================] - 281s 682us/sample - loss: 0.6778 - categorical_accuracy: 0.8515 - val_loss: 0.7400 - val_categorical_accuracy: 0.8521\n",
      "Epoch 38/100\n",
      "412059/412059 [==============================] - 278s 674us/sample - loss: 0.6726 - categorical_accuracy: 0.8522 - val_loss: 0.7275 - val_categorical_accuracy: 0.8523\n",
      "Epoch 39/100\n",
      "412059/412059 [==============================] - 274s 666us/sample - loss: 0.6689 - categorical_accuracy: 0.8532 - val_loss: 0.7233 - val_categorical_accuracy: 0.8523\n",
      "Epoch 40/100\n",
      "412059/412059 [==============================] - 283s 688us/sample - loss: 0.6647 - categorical_accuracy: 0.8537 - val_loss: 0.7377 - val_categorical_accuracy: 0.8530\n",
      "Epoch 41/100\n",
      "412059/412059 [==============================] - 279s 678us/sample - loss: 0.6608 - categorical_accuracy: 0.8546 - val_loss: 0.7171 - val_categorical_accuracy: 0.8517\n",
      "Epoch 42/100\n",
      "412059/412059 [==============================] - 279s 677us/sample - loss: 0.6565 - categorical_accuracy: 0.8554 - val_loss: 0.7118 - val_categorical_accuracy: 0.8537\n",
      "Epoch 43/100\n",
      "412059/412059 [==============================] - 279s 676us/sample - loss: 0.6543 - categorical_accuracy: 0.8564 - val_loss: 0.7134 - val_categorical_accuracy: 0.8537\n",
      "Epoch 44/100\n",
      "412059/412059 [==============================] - 279s 676us/sample - loss: 0.6497 - categorical_accuracy: 0.8570 - val_loss: 0.7325 - val_categorical_accuracy: 0.8547\n",
      "Epoch 45/100\n",
      "412059/412059 [==============================] - 277s 673us/sample - loss: 0.6455 - categorical_accuracy: 0.8577 - val_loss: 0.7061 - val_categorical_accuracy: 0.8542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/100\n",
      "412059/412059 [==============================] - 280s 680us/sample - loss: 0.6444 - categorical_accuracy: 0.8580 - val_loss: 0.7129 - val_categorical_accuracy: 0.8554\n",
      "Epoch 47/100\n",
      "412059/412059 [==============================] - 277s 672us/sample - loss: 0.6401 - categorical_accuracy: 0.8586 - val_loss: 0.7149 - val_categorical_accuracy: 0.8564\n",
      "Epoch 48/100\n",
      "412059/412059 [==============================] - 285s 692us/sample - loss: 0.6364 - categorical_accuracy: 0.8594 - val_loss: 0.7108 - val_categorical_accuracy: 0.8561\n",
      "Epoch 49/100\n",
      "412059/412059 [==============================] - 278s 675us/sample - loss: 0.6351 - categorical_accuracy: 0.8595 - val_loss: 0.7152 - val_categorical_accuracy: 0.8571\n",
      "Epoch 50/100\n",
      "412059/412059 [==============================] - 285s 691us/sample - loss: 0.6321 - categorical_accuracy: 0.8602 - val_loss: 0.7068 - val_categorical_accuracy: 0.8563\n",
      "Retrained Sherlock.\n",
      "Trained and saved new model.\n",
      "Finished at 2020-12-27 01:39:01.580431\n"
     ]
    }
   ],
   "source": [
    "print(f'Started at {datetime.now()}')\n",
    "\n",
    "train_sherlock(X_train, y_train, X_validation, y_validation, nn_id='retrained_sherlock3');\n",
    "\n",
    "print('Trained and saved new model.')\n",
    "print(f'Finished at {datetime.now()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions with a model\n",
    "If you want to use the pretrained Sherlock model `nn_id` set to \"sherlock\".\n",
    "\n",
    "If you want to use another model, you can use the identifier corresponding to that model.\n",
    "\n",
    "**Note**: There is a bug somewhere in the refactored code which affects the model predictions, this should be fixed soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = pd.read_parquet('../data/raw/test_labels.parquet').values.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = predict_sherlock(X_test, nn_id='retrained_sherlock3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction count 137353, type = <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(f'prediction count {len(predicted_labels)}, type = {type(predicted_labels)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8550958902652783"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size=len(y_test)\n",
    "\n",
    "# Should be fully deterministic too.\n",
    "f1_score(y_test[:size], predicted_labels[:size], average=\"weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(predicted_labels).nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(y_test).nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['affiliation', 'weight', 'jockey', 'religion', 'company', 'grades',\n",
       "       'area', 'component', 'company', 'brand', 'weight', 'genre',\n",
       "       'album', 'origin', 'description', 'status', 'credit', 'team Name',\n",
       "       'artist', 'address', 'age', 'album', 'club', 'description',\n",
       "       'family'], dtype=object)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_labels[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['affiliation', 'weight', 'jockey', 'religion', 'company', 'grades',\n",
       "       'area', 'component', 'company', 'manufacturer', 'weight', 'genre',\n",
       "       'album', 'origin', 'description', 'status', 'credit', 'team Name',\n",
       "       'artist', 'address', 'age', 'album', 'club', 'description',\n",
       "       'family'], dtype=object)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'address',\n",
       " 'affiliate',\n",
       " 'affiliation',\n",
       " 'age',\n",
       " 'album',\n",
       " 'area',\n",
       " 'artist',\n",
       " 'birth Date',\n",
       " 'birth Place',\n",
       " 'brand',\n",
       " 'capacity',\n",
       " 'category',\n",
       " 'city',\n",
       " 'class',\n",
       " 'classification',\n",
       " 'club',\n",
       " 'code',\n",
       " 'collection',\n",
       " 'command',\n",
       " 'company',\n",
       " 'component',\n",
       " 'continent',\n",
       " 'country',\n",
       " 'county',\n",
       " 'creator',\n",
       " 'credit',\n",
       " 'currency',\n",
       " 'day',\n",
       " 'depth',\n",
       " 'description',\n",
       " 'director',\n",
       " 'duration',\n",
       " 'education',\n",
       " 'elevation',\n",
       " 'family',\n",
       " 'file Size',\n",
       " 'format',\n",
       " 'gender',\n",
       " 'genre',\n",
       " 'grades',\n",
       " 'industry',\n",
       " 'isbn',\n",
       " 'jockey',\n",
       " 'language',\n",
       " 'location',\n",
       " 'manufacturer',\n",
       " 'name',\n",
       " 'nationality',\n",
       " 'notes',\n",
       " 'operator',\n",
       " 'order',\n",
       " 'organisation',\n",
       " 'origin',\n",
       " 'owner',\n",
       " 'person',\n",
       " 'plays',\n",
       " 'position',\n",
       " 'product',\n",
       " 'publisher',\n",
       " 'range',\n",
       " 'rank',\n",
       " 'ranking',\n",
       " 'region',\n",
       " 'religion',\n",
       " 'requirement',\n",
       " 'result',\n",
       " 'sales',\n",
       " 'service',\n",
       " 'sex',\n",
       " 'species',\n",
       " 'state',\n",
       " 'status',\n",
       " 'symbol',\n",
       " 'team',\n",
       " 'team Name',\n",
       " 'type',\n",
       " 'weight',\n",
       " 'year'}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total mismatches: 22468 (F1 score: 0.836800124917518)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('rank', 1277),\n",
       " ('name', 1083),\n",
       " ('position', 861),\n",
       " ('class', 840),\n",
       " ('plays', 773),\n",
       " ('description', 679),\n",
       " ('region', 667),\n",
       " ('location', 653),\n",
       " ('day', 567),\n",
       " ('artist', 541),\n",
       " ('type', 482),\n",
       " ('category', 475),\n",
       " ('team', 470),\n",
       " ('area', 464),\n",
       " ('team Name', 462),\n",
       " ('city', 459),\n",
       " ('notes', 456),\n",
       " ('album', 442),\n",
       " ('company', 406),\n",
       " ('owner', 400),\n",
       " ('code', 399),\n",
       " ('sex', 394),\n",
       " ('order', 386),\n",
       " ('product', 380),\n",
       " ('manufacturer', 376),\n",
       " ('age', 367),\n",
       " ('ranking', 348),\n",
       " ('status', 313),\n",
       " ('person', 288),\n",
       " ('credit', 277),\n",
       " ('country', 264),\n",
       " ('brand', 250),\n",
       " ('service', 245),\n",
       " ('county', 243),\n",
       " ('result', 240),\n",
       " ('year', 239),\n",
       " ('state', 232),\n",
       " ('component', 231),\n",
       " ('weight', 231),\n",
       " ('gender', 227),\n",
       " ('sales', 222),\n",
       " ('duration', 221),\n",
       " ('address', 219),\n",
       " ('club', 202),\n",
       " ('format', 196),\n",
       " ('origin', 185),\n",
       " ('range', 179),\n",
       " ('nationality', 154),\n",
       " ('director', 153),\n",
       " ('capacity', 149),\n",
       " ('family', 145),\n",
       " ('command', 138),\n",
       " ('publisher', 133),\n",
       " ('jockey', 133),\n",
       " ('classification', 131),\n",
       " ('symbol', 128),\n",
       " ('operator', 115),\n",
       " ('affiliation', 115),\n",
       " ('language', 102),\n",
       " ('requirement', 92),\n",
       " ('depth', 92),\n",
       " ('creator', 87),\n",
       " ('genre', 82),\n",
       " ('education', 78),\n",
       " ('species', 72),\n",
       " ('affiliate', 70),\n",
       " ('organisation', 67),\n",
       " ('industry', 65),\n",
       " ('elevation', 63),\n",
       " ('file Size', 56),\n",
       " ('birth Place', 43),\n",
       " ('religion', 42),\n",
       " ('collection', 41),\n",
       " ('continent', 27),\n",
       " ('currency', 25),\n",
       " ('grades', 22),\n",
       " ('isbn', 19),\n",
       " ('birth Date', 18)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "size = len(y_test)\n",
    "mismatches = list()\n",
    "\n",
    "for idx, k1 in enumerate(y_test[:size]):\n",
    "    k2 = predicted_labels[idx]\n",
    "\n",
    "    if k1 != k2:\n",
    "        mismatches.append(k1)\n",
    "#        if k1 in ('name'):\n",
    "#            print(f'[{idx}] expected \"{k1}\" but predicted \"{k2}\"')\n",
    "        \n",
    "f1 = f1_score(y_test[:size], predicted_labels[:size], average=\"weighted\")\n",
    "print(f'Total mismatches: {len(mismatches)} (F1 score: {f1})')\n",
    "\n",
    "data = Counter(mismatches)\n",
    "data.most_common()   # Returns all unique items and their counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples = pd.read_parquet('../data/raw/test_values.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted \"jockey\", actual label \"name\". Actual values:\n",
      "[['Alexander Steen', 'Patrik Berglund', 'Barret Jackman', 'Vladimir Sobotka', 'Chris Stewart', 'Kevin Shattenkirk', 'David Perron', 'Alex Pietrangelo', 'Andy McDonald']]\n"
     ]
    }
   ],
   "source": [
    "idx = 123758\n",
    "converted = test_samples.iloc[idx].apply(literal_eval).to_list()\n",
    "\n",
    "print(f'Predicted \"{predicted_labels[idx]}\", actual label \"{y_test[idx]}\". Actual values:\\n{converted}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions with preprocessed data using Sherlock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requires the data to be downloaded from Google Drive (see first step in notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_preprocessed = pd.read_parquet(\"../data/data/processed/X_test.parquet\")\n",
    "y_test_preprocessed = pd.read_parquet(\"../data/data/processed/y_test.parquet\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_preprocessed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test_preprocessed.to_csv('test_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_preprocessed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = predict_sherlock(X_test_preprocessed.head(n=25), 'sherlock')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_test_preprocessed.head(n=25), predicted_labels, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_preprocessed.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_keys = X_test_preprocessed.columns\n",
    "first_keys_str = ','.join(first_keys)\n",
    "\n",
    "keys = ','.join(X_test.columns)\n",
    "if first_keys_str == keys:\n",
    "    print('Keys are equal')\n",
    "else:\n",
    "    key_list = list(X_test.columns)\n",
    "\n",
    "    print(f'keys are NOT equal. k1 len={len(first_keys)}, k2 len={len(key_list)}')\n",
    "\n",
    "    for idx, k1 in enumerate(first_keys):\n",
    "        k2 = key_list[idx]\n",
    "\n",
    "        if k1 == k2:\n",
    "            print(f'{k1} == {k2}')\n",
    "        else:\n",
    "            print(f'{k1} != {k2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
