{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract features, retrain Sherlock and generate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script below first downloads the data (roughly 700K samples), then extract features from the raw data values. <br>\n",
    "If you want to skip this step, you can follow the steps below the feature extraction to load the preprocessed data, \n",
    "retrain Sherlock and generate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score\n",
    "import tensorflow as tf\n",
    "from ast import literal_eval\n",
    "\n",
    "from sherlock import helpers\n",
    "from sherlock.features.preprocessing import extract_features, convert_string_lists_to_lists, prepare_feature_extraction\n",
    "from sherlock.deploy.train_sherlock import train_sherlock\n",
    "from sherlock.deploy.predict_sherlock import predict_sherlock\n",
    "\n",
    "from pympler import muppy, summary\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data\n",
    "This will download the raw values and preprocessed files, the corresponding labels as well as a few other supporting files:\n",
    "- `download_data()` will download 3.6GB of data into the `data/` directory.\n",
    "- `prepare_feature_extraction()` will download +/- 800 MB of data into the `features/` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading the raw and preprocessed data into ../data/data.zip.\n",
      "Data was downloaded.\n",
      "Preparing feature extraction by downloading 2 files:\n",
      "        \n",
      " ../sherlock/features/glove.6B.50d.txt and \n",
      " ../sherlock/features/par_vec_trained_400.pkl.docvecs.vectors_docs.npy.\n",
      "        \n",
      "All files for extracting word and paragraph embeddings are present.\n"
     ]
    }
   ],
   "source": [
    "helpers.download_data()\n",
    "prepare_feature_extraction()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in raw data\n",
    "You can skip this step if you want to use a preprocessed data file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report memory usage (can be slow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_memory = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features\n",
    "It is important that the string-representations of lists are first converted into lists of strings.\n",
    "The labels should be a list of semantic types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "X_test_filename_csv = f'../data/work/test_{timestr}.csv'\n",
    "X_train_filename_csv = f'../data/work/train_{timestr}.csv'\n",
    "X_validation_filename_csv = f'../data/work/validation_{timestr}.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from functional import pseq\n",
    "from sherlock.functional import as_py_str, to_literal, randomise_sample, as_str_series, dropna, extract_features, normalise_string_whitespace, keys_on_first\n",
    "from pyarrow.parquet import ParquetFile\n",
    "\n",
    "\n",
    "def keys_to_csv(keys):\n",
    "    with io.StringIO() as output:\n",
    "        writer = csv.writer(output, quoting=csv.QUOTE_NONNUMERIC)\n",
    "        writer.writerow(keys)\n",
    "\n",
    "        return output.getvalue()\n",
    "    \n",
    "\n",
    "def load_parquet_values(path):\n",
    "    pf = ParquetFile(source = path)\n",
    "    row_df = pf.read_row_group(0)\n",
    "    \n",
    "    return row_df['values']\n",
    "\n",
    "\n",
    "def extract_features_to_csv(output_path, parquet_values):\n",
    "    # incompatible with keys_on_first\n",
    "    verify_keys = False\n",
    "    first_keys = None\n",
    "    i = 0\n",
    "\n",
    "    start = datetime.now()\n",
    "\n",
    "    print(f'Starting {output_path} at {start}')\n",
    "\n",
    "    with open(output_path, \"w\") as outfile:\n",
    "        # Comparable performance with using pool.imap directly, but the code is *much* cleaner\n",
    "        for keys, values_str in pseq(parquet_values, processes=6, partition_size=10)\\\n",
    "            .map(as_py_str)\\\n",
    "            .map(to_literal)\\\n",
    "            .map(randomise_sample)\\\n",
    "            .map(normalise_string_whitespace)\\\n",
    "            .map(as_str_series)\\\n",
    "            .map(dropna)\\\n",
    "            .map(extract_features)\\\n",
    "            .map(keys_on_first): # to-do: make this function a partial, and pass in the verify_keys\n",
    "                i = i+1\n",
    "\n",
    "                if first_keys is None:\n",
    "                    first_keys = keys\n",
    "                    first_keys_str = keys_to_csv(keys)\n",
    "\n",
    "                    print(f'Exporting {len(first_keys)} column features')\n",
    "                    \n",
    "                    outfile.write(keys_to_csv(keys))\n",
    "                    outfile.write('\\n')\n",
    "                elif verify_keys: # incompatible with keys_on_first\n",
    "                    keys_str = ','.join(keys)\n",
    "                    if first_keys_str != keys_str:\n",
    "                        key_list = list(keys)\n",
    "\n",
    "                        print(f'keys are NOT equal. k1 len={len(first_keys)}, k2 len={len(keys)}')\n",
    "\n",
    "                        for idx, k1 in enumerate(first_keys):\n",
    "                            k2 = key_list[idx]\n",
    "\n",
    "                            if k1 != k2:\n",
    "                                print(f'{k1} != {k2}')\n",
    "\n",
    "                outfile.write(values_str)\n",
    "                outfile.write('\\n')\n",
    "\n",
    "    print(f'Finished. Processed {i} rows in {datetime.now() - start}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXTRACT FEATURES TO CSV (NEW METHOD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing feature extraction by downloading 2 files:\n",
      "        \n",
      " ../sherlock/features/glove.6B.50d.txt and \n",
      " ../sherlock/features/par_vec_trained_400.pkl.docvecs.vectors_docs.npy.\n",
      "        \n",
      "All files for extracting word and paragraph embeddings are present.\n",
      "Initialising word embeddings\n",
      "Initialise Word Embeddings process took 0:00:05.617424 seconds.\n",
      "Initialise Doc2Vec Model, 400 dim, process took 0:00:00.345168 seconds. (filename = ../sherlock/features/par_vec_trained_400.pkl)\n"
     ]
    }
   ],
   "source": [
    "# ensure embedding initialisation is outside of timing for extract_features\n",
    "from sherlock.features.word_embeddings import initialise_word_embeddings\n",
    "from sherlock.features.paragraph_vectors import initialise_pretrained_model\n",
    "from sherlock.features.preprocessing import prepare_feature_extraction\n",
    "\n",
    "prepare_feature_extraction()\n",
    "initialise_word_embeddings()\n",
    "initialise_pretrained_model(400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting ../data/work/test_20201228-095113.csv at 2020-12-28 09:51:20.027644\n",
      "Exporting 1578 column features\n",
      "Finished. Processed 137353 rows in 0:17:38.860040\n"
     ]
    }
   ],
   "source": [
    "values = load_parquet_values(\"../data/raw/test_values.parquet\")\n",
    "\n",
    "extract_features_to_csv(X_test_filename_csv, values)\n",
    "\n",
    "values = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished at 2020-12-28 10:08:59.044344\n"
     ]
    }
   ],
   "source": [
    "print(f'Finished at {datetime.now()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAIN SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting ../data/work/train_20201228-095113.csv at 2020-12-28 10:09:01.788163\n",
      "Exporting 1578 column features\n",
      "Finished. Processed 412059 rows in 0:50:56.331647\n"
     ]
    }
   ],
   "source": [
    "values = load_parquet_values(\"../data/raw/train_values.parquet\")\n",
    "\n",
    "extract_features_to_csv(X_train_filename_csv, values)\n",
    "\n",
    "values = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished at 2020-12-28 10:59:58.393430\n"
     ]
    }
   ],
   "source": [
    "print(f'Finished at {datetime.now()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VALIDATION SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting ../data/work/validation_20201228-095113.csv at 2020-12-28 10:59:59.267311\n",
      "Exporting 1578 column features\n",
      "Finished. Processed 137353 rows in 0:16:53.218644\n"
     ]
    }
   ],
   "source": [
    "values = load_parquet_values(\"../data/raw/val_values.parquet\")\n",
    "\n",
    "extract_features_to_csv(X_validation_filename_csv, values)\n",
    "\n",
    "values = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished at 2020-12-28 11:16:52.589097\n"
     ]
    }
   ],
   "source": [
    "print(f'Finished at {datetime.now()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXTRACT FEATURES TO CSV (**OLD** METHOD) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST SET (OLD METHOD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if report_memory:\n",
    "    all_objects = muppy.get_objects()\n",
    "    sum1 = summary.summarize(all_objects)\n",
    "    # Prints out a summary of the large objects\n",
    "    summary.print_(sum1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples = pd.read_parquet('../data/raw/test_values.parquet')\n",
    "test_labels = pd.read_parquet('../data/raw/test_labels.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples_converted, y_test = convert_string_lists_to_lists(test_samples, test_labels, \"values\", \"type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# free memory\n",
    "test_samples = None\n",
    "test_labels = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples_converted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output \"head\" \n",
    "y_test[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Given that feature extraction can take long, we only take the first 100 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_subset = y_test[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure embedding initialisation is outside of timing for extract_features\n",
    "from sherlock.features.word_embeddings import initialise_word_embeddings\n",
    "from sherlock.features.paragraph_vectors import initialise_pretrained_model\n",
    "\n",
    "initialise_word_embeddings()\n",
    "initialise_pretrained_model(400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if report_memory:\n",
    "    all_objects = muppy.get_objects()\n",
    "    sum1 = summary.summarize(all_objects)\n",
    "    # Prints out a summary of the large objects\n",
    "    summary.print_(sum1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "#X_test = extract_features(test_samples_converted.head(n=100))\n",
    "extract_features(X_test_filename, test_samples_converted.head(n=100))\n",
    "\n",
    "print(f'Extract Features (test) process took {datetime.now() - start} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples_converted = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if report_memory:\n",
    "    all_objects = muppy.get_objects()\n",
    "    sum1 = summary.summarize(all_objects)\n",
    "    # Prints out a summary of the large objects\n",
    "    summary.print_(sum1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# over all, without memory management\n",
    "# Extract Features (test) process took 3:40:25.799880 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline\n",
    "# Extract Features (test) process took 0:11:04.137081 seconds.\n",
    "\n",
    "# Iterations\n",
    "# Extract Features (test) process took 0:00:56.671353 seconds. (cache word embeddings)\n",
    "# Extract Features (test) process took 0:00:13.523261 seconds. (cache Doc2Vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAIN SET (OLD METHOD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples = pd.read_parquet('../data/raw/train_values.parquet')\n",
    "train_labels = pd.read_parquet('../data/raw/train_labels.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples_converted, y_train = convert_string_lists_to_lists(train_samples, train_labels, \"values\", \"type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# free memory\n",
    "train_samples = None\n",
    "train_labels = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_subset = y_train[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "extract_features(X_train_filename, train_samples_converted)\n",
    "\n",
    "print(f'Extract Features (train) process took {datetime.now() - start} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples_converted = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VALIDATION SET (OLD METHOD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_samples = pd.read_parquet('../data/raw/val_values.parquet')\n",
    "validation_labels = pd.read_parquet('../data/raw/val_labels.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_samples_converted, y_validation = convert_string_lists_to_lists(validation_samples, validation_labels, \"values\", \"type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# free memory\n",
    "validation_samples = None\n",
    "validation_labels = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_validation_subset = y_validation[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "extract_features(X_validation_filename, validation_samples_converted)\n",
    "\n",
    "print(f'Extract Features (validation) process took {datetime.now() - start} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_samples_converted = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Locally Processed Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestr = '20201224-105345'\n",
    "\n",
    "X_test_filename_csv = f'../data/work/test_{timestr}.csv'\n",
    "X_train_filename_csv = f'../data/work/train_{timestr}.csv'\n",
    "X_validation_filename_csv = f'../data/work/validation_{timestr}.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Features (test) process took 0:00:30.782784 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "X_test = pd.read_csv(X_test_filename_csv, dtype=np.float32)\n",
    "\n",
    "print(f'Load Features (test) process took {datetime.now() - start} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_[0]-agg-any</th>\n",
       "      <th>n_[0]-agg-all</th>\n",
       "      <th>n_[0]-agg-mean</th>\n",
       "      <th>n_[0]-agg-var</th>\n",
       "      <th>n_[0]-agg-min</th>\n",
       "      <th>n_[0]-agg-max</th>\n",
       "      <th>n_[0]-agg-median</th>\n",
       "      <th>n_[0]-agg-sum</th>\n",
       "      <th>n_[0]-agg-kurtosis</th>\n",
       "      <th>n_[0]-agg-skewness</th>\n",
       "      <th>...</th>\n",
       "      <th>par_vec_390</th>\n",
       "      <th>par_vec_391</th>\n",
       "      <th>par_vec_392</th>\n",
       "      <th>par_vec_393</th>\n",
       "      <th>par_vec_394</th>\n",
       "      <th>par_vec_395</th>\n",
       "      <th>par_vec_396</th>\n",
       "      <th>par_vec_397</th>\n",
       "      <th>par_vec_398</th>\n",
       "      <th>par_vec_399</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000858</td>\n",
       "      <td>0.000760</td>\n",
       "      <td>-0.000477</td>\n",
       "      <td>-0.000894</td>\n",
       "      <td>0.000838</td>\n",
       "      <td>-0.001224</td>\n",
       "      <td>0.000353</td>\n",
       "      <td>-0.000655</td>\n",
       "      <td>0.000566</td>\n",
       "      <td>0.000537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.337950</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.742677</td>\n",
       "      <td>1.32687</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000528</td>\n",
       "      <td>-0.000353</td>\n",
       "      <td>-0.000504</td>\n",
       "      <td>0.000945</td>\n",
       "      <td>-0.000456</td>\n",
       "      <td>-0.000986</td>\n",
       "      <td>-0.000151</td>\n",
       "      <td>0.000882</td>\n",
       "      <td>-0.000310</td>\n",
       "      <td>-0.000562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000518</td>\n",
       "      <td>-0.000046</td>\n",
       "      <td>-0.001005</td>\n",
       "      <td>-0.000698</td>\n",
       "      <td>0.000380</td>\n",
       "      <td>-0.000641</td>\n",
       "      <td>-0.000597</td>\n",
       "      <td>0.001205</td>\n",
       "      <td>-0.000855</td>\n",
       "      <td>-0.000297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000597</td>\n",
       "      <td>0.000691</td>\n",
       "      <td>-0.000288</td>\n",
       "      <td>-0.000656</td>\n",
       "      <td>0.001050</td>\n",
       "      <td>0.000390</td>\n",
       "      <td>-0.000236</td>\n",
       "      <td>-0.000667</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>0.000689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.020080</td>\n",
       "      <td>0.035741</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>96.521599</td>\n",
       "      <td>9.78415</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000113</td>\n",
       "      <td>-0.000873</td>\n",
       "      <td>0.000573</td>\n",
       "      <td>0.001029</td>\n",
       "      <td>0.000660</td>\n",
       "      <td>0.000528</td>\n",
       "      <td>-0.000259</td>\n",
       "      <td>-0.000650</td>\n",
       "      <td>0.001239</td>\n",
       "      <td>-0.000386</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1578 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_[0]-agg-any  n_[0]-agg-all  n_[0]-agg-mean  n_[0]-agg-var  n_[0]-agg-min  \\\n",
       "0            0.0            0.0        0.000000       0.000000            0.0   \n",
       "1            1.0            0.0        0.368421       0.337950            0.0   \n",
       "2            0.0            0.0        0.000000       0.000000            0.0   \n",
       "3            0.0            0.0        0.000000       0.000000            0.0   \n",
       "4            1.0            0.0        0.020080       0.035741            0.0   \n",
       "\n",
       "   n_[0]-agg-max  n_[0]-agg-median  n_[0]-agg-sum  n_[0]-agg-kurtosis  \\\n",
       "0            0.0               0.0            0.0           -3.000000   \n",
       "1            2.0               0.0            7.0            0.742677   \n",
       "2            0.0               0.0            0.0           -3.000000   \n",
       "3            0.0               0.0            0.0           -3.000000   \n",
       "4            2.0               0.0            5.0           96.521599   \n",
       "\n",
       "   n_[0]-agg-skewness  ...  par_vec_390  par_vec_391  par_vec_392  \\\n",
       "0             0.00000  ...     0.000858     0.000760    -0.000477   \n",
       "1             1.32687  ...     0.000528    -0.000353    -0.000504   \n",
       "2             0.00000  ...     0.000518    -0.000046    -0.001005   \n",
       "3             0.00000  ...    -0.000597     0.000691    -0.000288   \n",
       "4             9.78415  ...    -0.000113    -0.000873     0.000573   \n",
       "\n",
       "   par_vec_393  par_vec_394  par_vec_395  par_vec_396  par_vec_397  \\\n",
       "0    -0.000894     0.000838    -0.001224     0.000353    -0.000655   \n",
       "1     0.000945    -0.000456    -0.000986    -0.000151     0.000882   \n",
       "2    -0.000698     0.000380    -0.000641    -0.000597     0.001205   \n",
       "3    -0.000656     0.001050     0.000390    -0.000236    -0.000667   \n",
       "4     0.001029     0.000660     0.000528    -0.000259    -0.000650   \n",
       "\n",
       "   par_vec_398  par_vec_399  \n",
       "0     0.000566     0.000537  \n",
       "1    -0.000310    -0.000562  \n",
       "2    -0.000855    -0.000297  \n",
       "3     0.000136     0.000689  \n",
       "4     0.001239    -0.000386  \n",
       "\n",
       "[5 rows x 1578 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Features (train) process took 0:01:34.038627 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "X_train = pd.read_csv(X_train_filename_csv, dtype=np.float32)\n",
    "\n",
    "print(f'Load Features (train) process took {datetime.now() - start} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_[0]-agg-any</th>\n",
       "      <th>n_[0]-agg-all</th>\n",
       "      <th>n_[0]-agg-mean</th>\n",
       "      <th>n_[0]-agg-var</th>\n",
       "      <th>n_[0]-agg-min</th>\n",
       "      <th>n_[0]-agg-max</th>\n",
       "      <th>n_[0]-agg-median</th>\n",
       "      <th>n_[0]-agg-sum</th>\n",
       "      <th>n_[0]-agg-kurtosis</th>\n",
       "      <th>n_[0]-agg-skewness</th>\n",
       "      <th>...</th>\n",
       "      <th>par_vec_390</th>\n",
       "      <th>par_vec_391</th>\n",
       "      <th>par_vec_392</th>\n",
       "      <th>par_vec_393</th>\n",
       "      <th>par_vec_394</th>\n",
       "      <th>par_vec_395</th>\n",
       "      <th>par_vec_396</th>\n",
       "      <th>par_vec_397</th>\n",
       "      <th>par_vec_398</th>\n",
       "      <th>par_vec_399</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000187</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.001129</td>\n",
       "      <td>-0.000150</td>\n",
       "      <td>0.000268</td>\n",
       "      <td>0.000933</td>\n",
       "      <td>0.000559</td>\n",
       "      <td>-0.000327</td>\n",
       "      <td>0.000676</td>\n",
       "      <td>-0.001160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000521</td>\n",
       "      <td>0.001054</td>\n",
       "      <td>0.001103</td>\n",
       "      <td>-0.001192</td>\n",
       "      <td>0.000949</td>\n",
       "      <td>0.000982</td>\n",
       "      <td>-0.000710</td>\n",
       "      <td>-0.000395</td>\n",
       "      <td>0.000786</td>\n",
       "      <td>0.000155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000786</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>-0.000540</td>\n",
       "      <td>0.001154</td>\n",
       "      <td>0.000818</td>\n",
       "      <td>0.000298</td>\n",
       "      <td>-0.000822</td>\n",
       "      <td>-0.001179</td>\n",
       "      <td>0.000924</td>\n",
       "      <td>0.000990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000516</td>\n",
       "      <td>0.000566</td>\n",
       "      <td>-0.001198</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>-0.000356</td>\n",
       "      <td>-0.000701</td>\n",
       "      <td>-0.000855</td>\n",
       "      <td>-0.000915</td>\n",
       "      <td>0.000859</td>\n",
       "      <td>0.001127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001070</td>\n",
       "      <td>0.000864</td>\n",
       "      <td>0.000288</td>\n",
       "      <td>-0.000666</td>\n",
       "      <td>0.000863</td>\n",
       "      <td>-0.001102</td>\n",
       "      <td>-0.000942</td>\n",
       "      <td>-0.000307</td>\n",
       "      <td>-0.000346</td>\n",
       "      <td>0.000427</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1578 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_[0]-agg-any  n_[0]-agg-all  n_[0]-agg-mean  n_[0]-agg-var  n_[0]-agg-min  \\\n",
       "0            0.0            0.0             0.0            0.0            0.0   \n",
       "1            0.0            0.0             0.0            0.0            0.0   \n",
       "2            0.0            0.0             0.0            0.0            0.0   \n",
       "3            0.0            0.0             0.0            0.0            0.0   \n",
       "4            0.0            0.0             0.0            0.0            0.0   \n",
       "\n",
       "   n_[0]-agg-max  n_[0]-agg-median  n_[0]-agg-sum  n_[0]-agg-kurtosis  \\\n",
       "0            0.0               0.0            0.0                -3.0   \n",
       "1            0.0               0.0            0.0                -3.0   \n",
       "2            0.0               0.0            0.0                -3.0   \n",
       "3            0.0               0.0            0.0                -3.0   \n",
       "4            0.0               0.0            0.0                -3.0   \n",
       "\n",
       "   n_[0]-agg-skewness  ...  par_vec_390  par_vec_391  par_vec_392  \\\n",
       "0                 0.0  ...    -0.000187     0.000029     0.001129   \n",
       "1                 0.0  ...    -0.000521     0.001054     0.001103   \n",
       "2                 0.0  ...    -0.000786     0.000048    -0.000540   \n",
       "3                 0.0  ...    -0.000516     0.000566    -0.001198   \n",
       "4                 0.0  ...     0.001070     0.000864     0.000288   \n",
       "\n",
       "   par_vec_393  par_vec_394  par_vec_395  par_vec_396  par_vec_397  \\\n",
       "0    -0.000150     0.000268     0.000933     0.000559    -0.000327   \n",
       "1    -0.001192     0.000949     0.000982    -0.000710    -0.000395   \n",
       "2     0.001154     0.000818     0.000298    -0.000822    -0.001179   \n",
       "3     0.000007    -0.000356    -0.000701    -0.000855    -0.000915   \n",
       "4    -0.000666     0.000863    -0.001102    -0.000942    -0.000307   \n",
       "\n",
       "   par_vec_398  par_vec_399  \n",
       "0     0.000676    -0.001160  \n",
       "1     0.000786     0.000155  \n",
       "2     0.000924     0.000990  \n",
       "3     0.000859     0.001127  \n",
       "4    -0.000346     0.000427  \n",
       "\n",
       "[5 rows x 1578 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Features (validation) process took 0:00:31.175299 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "X_validation = pd.read_csv(X_validation_filename_csv, dtype=np.float32)\n",
    "\n",
    "print(f'Load Features (validation) process took {datetime.now() - start} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_[0]-agg-any</th>\n",
       "      <th>n_[0]-agg-all</th>\n",
       "      <th>n_[0]-agg-mean</th>\n",
       "      <th>n_[0]-agg-var</th>\n",
       "      <th>n_[0]-agg-min</th>\n",
       "      <th>n_[0]-agg-max</th>\n",
       "      <th>n_[0]-agg-median</th>\n",
       "      <th>n_[0]-agg-sum</th>\n",
       "      <th>n_[0]-agg-kurtosis</th>\n",
       "      <th>n_[0]-agg-skewness</th>\n",
       "      <th>...</th>\n",
       "      <th>par_vec_390</th>\n",
       "      <th>par_vec_391</th>\n",
       "      <th>par_vec_392</th>\n",
       "      <th>par_vec_393</th>\n",
       "      <th>par_vec_394</th>\n",
       "      <th>par_vec_395</th>\n",
       "      <th>par_vec_396</th>\n",
       "      <th>par_vec_397</th>\n",
       "      <th>par_vec_398</th>\n",
       "      <th>par_vec_399</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000041</td>\n",
       "      <td>-0.000604</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>-0.001002</td>\n",
       "      <td>-0.000816</td>\n",
       "      <td>-0.000517</td>\n",
       "      <td>-0.000162</td>\n",
       "      <td>0.000447</td>\n",
       "      <td>-0.000991</td>\n",
       "      <td>-0.001119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000346</td>\n",
       "      <td>0.000752</td>\n",
       "      <td>0.000917</td>\n",
       "      <td>0.000994</td>\n",
       "      <td>-0.001099</td>\n",
       "      <td>0.000208</td>\n",
       "      <td>0.000226</td>\n",
       "      <td>0.000293</td>\n",
       "      <td>-0.001147</td>\n",
       "      <td>0.000102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-0.666667</td>\n",
       "      <td>1.15470</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000214</td>\n",
       "      <td>0.000341</td>\n",
       "      <td>-0.000271</td>\n",
       "      <td>0.000169</td>\n",
       "      <td>-0.000431</td>\n",
       "      <td>-0.000201</td>\n",
       "      <td>-0.000945</td>\n",
       "      <td>-0.001021</td>\n",
       "      <td>0.000314</td>\n",
       "      <td>0.000636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>-0.000419</td>\n",
       "      <td>-0.000654</td>\n",
       "      <td>-0.000038</td>\n",
       "      <td>-0.000995</td>\n",
       "      <td>-0.000175</td>\n",
       "      <td>-0.000871</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.000520</td>\n",
       "      <td>0.000551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.09434</td>\n",
       "      <td>0.311855</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>42.275002</td>\n",
       "      <td>6.53059</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000664</td>\n",
       "      <td>-0.000761</td>\n",
       "      <td>-0.000600</td>\n",
       "      <td>-0.000969</td>\n",
       "      <td>-0.000591</td>\n",
       "      <td>0.000630</td>\n",
       "      <td>0.000505</td>\n",
       "      <td>0.000988</td>\n",
       "      <td>0.000451</td>\n",
       "      <td>-0.000105</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1578 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   n_[0]-agg-any  n_[0]-agg-all  n_[0]-agg-mean  n_[0]-agg-var  n_[0]-agg-min  \\\n",
       "0            0.0            0.0         0.00000       0.000000            0.0   \n",
       "1            0.0            0.0         0.00000       0.000000            0.0   \n",
       "2            1.0            0.0         0.25000       0.187500            0.0   \n",
       "3            0.0            0.0         0.00000       0.000000            0.0   \n",
       "4            1.0            0.0         0.09434       0.311855            0.0   \n",
       "\n",
       "   n_[0]-agg-max  n_[0]-agg-median  n_[0]-agg-sum  n_[0]-agg-kurtosis  \\\n",
       "0            0.0               0.0            0.0           -3.000000   \n",
       "1            0.0               0.0            0.0           -3.000000   \n",
       "2            1.0               0.0           10.0           -0.666667   \n",
       "3            0.0               0.0            0.0           -3.000000   \n",
       "4            4.0               0.0            5.0           42.275002   \n",
       "\n",
       "   n_[0]-agg-skewness  ...  par_vec_390  par_vec_391  par_vec_392  \\\n",
       "0             0.00000  ...    -0.000041    -0.000604     0.000047   \n",
       "1             0.00000  ...     0.000346     0.000752     0.000917   \n",
       "2             1.15470  ...    -0.000214     0.000341    -0.000271   \n",
       "3             0.00000  ...     0.000039    -0.000419    -0.000654   \n",
       "4             6.53059  ...    -0.000664    -0.000761    -0.000600   \n",
       "\n",
       "   par_vec_393  par_vec_394  par_vec_395  par_vec_396  par_vec_397  \\\n",
       "0    -0.001002    -0.000816    -0.000517    -0.000162     0.000447   \n",
       "1     0.000994    -0.001099     0.000208     0.000226     0.000293   \n",
       "2     0.000169    -0.000431    -0.000201    -0.000945    -0.001021   \n",
       "3    -0.000038    -0.000995    -0.000175    -0.000871     0.000118   \n",
       "4    -0.000969    -0.000591     0.000630     0.000505     0.000988   \n",
       "\n",
       "   par_vec_398  par_vec_399  \n",
       "0    -0.000991    -0.001119  \n",
       "1    -0.001147     0.000102  \n",
       "2     0.000314     0.000636  \n",
       "3     0.000520     0.000551  \n",
       "4     0.000451    -0.000105  \n",
       "\n",
       "[5 rows x 1578 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_validation.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impute NaN values with feature means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transpose process took 0:00:19.554785 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "train_columns_means = pd.DataFrame(X_train.mean()).transpose()\n",
    "\n",
    "print(f'Transpose process took {datetime.now() - start} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FillNA process took 0:00:01.880830 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "X_train.fillna(train_columns_means.iloc[0], inplace=True)\n",
    "X_validation.fillna(train_columns_means.iloc[0], inplace=True)\n",
    "X_test.fillna(train_columns_means.iloc[0], inplace=True)\n",
    "\n",
    "train_columns_means=None\n",
    "\n",
    "print(f'FillNA process took {datetime.now() - start} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.now()\n",
    "\n",
    "X_train.to_parquet('train.parquet', engine='pyarrow', compression='snappy')\n",
    "X_validation.to_parquet('validation.parquet', engine='pyarrow', compression='snappy')\n",
    "X_test.to_parquet('test.parquet', engine='pyarrow', compression='snappy')\n",
    "\n",
    "print(f'Save parquet process took {datetime.now() - start} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished at 2020-12-28 11:20:47.922207\n"
     ]
    }
   ],
   "source": [
    "print(f'Finished at {datetime.now()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restore Pickled Session ready for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('train.csv')\n",
    "X_validation = pd.read_csv('validation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pd.read_parquet('../data/raw/train_labels.parquet').values.flatten()\n",
    "y_validation = pd.read_parquet('../data/raw/val_labels.parquet').values.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrain sherlock\n",
    "The model can be retrained using the code below. The model is currently restricted to be trained on 78 classes, the code of the model architecture will soon be added for adjusting this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Started at {datetime.now()}')\n",
    "\n",
    "train_sherlock(X_train, y_train, X_validation, y_validation, nn_id='retrained_sherlock3');\n",
    "\n",
    "print('Trained and saved new model.')\n",
    "print(f'Finished at {datetime.now()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions with a model\n",
    "If you want to use the pretrained Sherlock model `nn_id` set to \"sherlock\".\n",
    "\n",
    "If you want to use another model, you can use the identifier corresponding to that model.\n",
    "\n",
    "**Note**: There is a bug somewhere in the refactored code which affects the model predictions, this should be fixed soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = pd.read_parquet('../data/raw/test_labels.parquet').values.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = predict_sherlock(X_test, nn_id='retrained_sherlock3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'prediction count {len(predicted_labels)}, type = {type(predicted_labels)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "size=len(y_test)\n",
    "\n",
    "# Should be fully deterministic too.\n",
    "f1_score(y_test[:size], predicted_labels[:size], average=\"weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(predicted_labels).nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(y_test).nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "size = len(y_test)\n",
    "mismatches = list()\n",
    "\n",
    "for idx, k1 in enumerate(y_test[:size]):\n",
    "    k2 = predicted_labels[idx]\n",
    "\n",
    "    if k1 != k2:\n",
    "        mismatches.append(k1)\n",
    "#        if k1 in ('name'):\n",
    "#            print(f'[{idx}] expected \"{k1}\" but predicted \"{k2}\"')\n",
    "        \n",
    "f1 = f1_score(y_test[:size], predicted_labels[:size], average=\"weighted\")\n",
    "print(f'Total mismatches: {len(mismatches)} (F1 score: {f1})')\n",
    "\n",
    "data = Counter(mismatches)\n",
    "data.most_common()   # Returns all unique items and their counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples = pd.read_parquet('../data/raw/test_values.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 123758\n",
    "converted = test_samples.iloc[idx].apply(literal_eval).to_list()\n",
    "\n",
    "print(f'Predicted \"{predicted_labels[idx]}\", actual label \"{y_test[idx]}\". Actual values:\\n{converted}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate predictions with preprocessed data using Sherlock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requires the data to be downloaded from Google Drive (see first step in notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_preprocessed = pd.read_parquet(\"../data/data/processed/X_test.parquet\")\n",
    "y_test_preprocessed = pd.read_parquet(\"../data/data/processed/y_test.parquet\").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_preprocessed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test_preprocessed.to_csv('test_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_preprocessed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = predict_sherlock(X_test_preprocessed.head(n=25), 'sherlock')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_test_preprocessed.head(n=25), predicted_labels, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_preprocessed.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_keys = X_test_preprocessed.columns\n",
    "first_keys_str = ','.join(first_keys)\n",
    "\n",
    "keys = ','.join(X_test.columns)\n",
    "if first_keys_str == keys:\n",
    "    print('Keys are equal')\n",
    "else:\n",
    "    key_list = list(X_test.columns)\n",
    "\n",
    "    print(f'keys are NOT equal. k1 len={len(first_keys)}, k2 len={len(key_list)}')\n",
    "\n",
    "    for idx, k1 in enumerate(first_keys):\n",
    "        k2 = key_list[idx]\n",
    "\n",
    "        if k1 == k2:\n",
    "            print(f'{k1} == {k2}')\n",
    "        else:\n",
    "            print(f'{k1} != {k2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
