{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score\n",
    "import tensorflow as tf\n",
    "from ast import literal_eval\n",
    "\n",
    "from sherlock import helpers\n",
    "from sherlock.features.preprocessing import extract_features, convert_string_lists_to_lists, prepare_feature_extraction\n",
    "from sherlock.deploy.train_sherlock import train_sherlock\n",
    "from sherlock.deploy.predict_sherlock import predict_sherlock\n",
    "from sherlock.features.word_embeddings import initialise_word_embeddings\n",
    "from sherlock.features.paragraph_vectors import initialise_pretrained_model\n",
    "\n",
    "from pympler import muppy, summary\n",
    "from datetime import datetime\n",
    "\n",
    "from pyarrow.parquet import ParquetFile\n",
    "\n",
    "from sherlock.functional import as_py_str, to_literal, randomise_sample, as_str_series, dropna, extract_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing feature extraction by downloading 2 files:\n",
      "        \n",
      " ../sherlock/features/glove.6B.50d.txt and \n",
      " ../sherlock/features/par_vec_trained_400.pkl.docvecs.vectors_docs.npy.\n",
      "        \n",
      "All files for extracting word and paragraph embeddings are present.\n",
      "Initialising word embeddings\n",
      "Initialise Word Embeddings process took 0:00:06.065168 seconds.\n",
      "Initialise Doc2Vec Model, 400 dim, process took 0:00:01.532057 seconds.\n"
     ]
    }
   ],
   "source": [
    "prepare_feature_extraction()\n",
    "initialise_word_embeddings()\n",
    "initialise_pretrained_model(400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../data/raw/test_values.parquet\"\n",
    "pf = ParquetFile(source = path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_df = pf.read_row_group(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting 1588 column features\n",
      "Finished. Processed 137353 rows in 0:26:02.218227\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "verify_keys = False\n",
    "first_keys = None\n",
    "i = 0\n",
    "\n",
    "start = datetime.now()\n",
    "\n",
    "with Pool(processes=6) as pool:\n",
    "    with open('test.csv', \"w\") as outfile:\n",
    "        csvwriter = csv.writer(outfile)\n",
    "\n",
    "        for features in pool.imap(extract_features,\n",
    "                                  map(dropna,\n",
    "                                      map(as_str_series,\n",
    "                                          map(randomise_sample,\n",
    "                                              pool.imap(to_literal,\n",
    "                                                       map(as_py_str, row_df['values'])))))):\n",
    "            i = i+1\n",
    "\n",
    "            keys=features.keys()\n",
    "\n",
    "            if first_keys is None:\n",
    "                first_keys = keys\n",
    "                first_keys_str = ','.join(keys)\n",
    "\n",
    "                print(f'Exporting {len(first_keys)} column features')\n",
    "\n",
    "                csvwriter.writerow(first_keys)\n",
    "            elif verify_keys:\n",
    "                keys_str = ','.join(keys)\n",
    "                if first_keys_str != keys_str:\n",
    "                    key_list = list(keys)\n",
    "\n",
    "                    print(f'keys are NOT equal. k1 len={len(first_keys)}, k2 len={len(keys)}')\n",
    "\n",
    "                    for idx, k1 in enumerate(first_keys):\n",
    "                        k2 = key_list[idx]\n",
    "\n",
    "                        if k1 != k2:\n",
    "                            print(f'{k1} != {k2}')\n",
    "\n",
    "            csvwriter.writerow(list(features.values()))\n",
    "\n",
    "print(f'Finished. Processed {i} rows in {datetime.now() - start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting at 2020-12-22 09:02:16.363139\n",
      "Exporting 1588 column features\n",
      "Finished. Processed 137353 rows in 0:23:55.255376\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "from functional import pseq\n",
    "\n",
    "verify_keys = False\n",
    "first_keys = None\n",
    "i = 0\n",
    "\n",
    "start = datetime.now()\n",
    "\n",
    "print(f'Starting at {start}')\n",
    "\n",
    "with open('test.csv', \"w\") as outfile:\n",
    "    csvwriter = csv.writer(outfile)\n",
    "        \n",
    "    # Comparable performance with using pool.imap directly, but the code is *much* cleaner\n",
    "    for features in pseq(row_df['values'], processes=6, partition_size=10)\\\n",
    "        .map(as_py_str)\\\n",
    "        .map(to_literal)\\\n",
    "        .map(randomised_sample)\\\n",
    "        .map(as_str_series)\\\n",
    "        .map(dropna)\\\n",
    "        .map(extract_features):\n",
    "            i = i+1\n",
    "            \n",
    "            keys=features.keys()\n",
    "\n",
    "            if first_keys is None:\n",
    "                first_keys = keys\n",
    "                first_keys_str = ','.join(keys)\n",
    "\n",
    "                print(f'Exporting {len(first_keys)} column features')\n",
    "\n",
    "                csvwriter.writerow(first_keys)\n",
    "            elif verify_keys:\n",
    "                keys_str = ','.join(keys)\n",
    "                if first_keys_str != keys_str:\n",
    "                    key_list = list(keys)\n",
    "\n",
    "                    print(f'keys are NOT equal. k1 len={len(first_keys)}, k2 len={len(keys)}')\n",
    "\n",
    "                    for idx, k1 in enumerate(first_keys):\n",
    "                        k2 = key_list[idx]\n",
    "\n",
    "                        if k1 != k2:\n",
    "                            print(f'{k1} != {k2}')\n",
    "\n",
    "            csvwriter.writerow(list(features.values()))\n",
    "\n",
    "print(f'Finished. Processed {i} rows in {datetime.now() - start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting at 2020-12-23 09:32:15.332880\n",
      "Waiting for background thread to complete\n",
      "Processed 137353 items\n",
      "Finished. Processed in 0:24:12.906544\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "from functional import pseq\n",
    "from sherlock.functional import values_only, black_hole\n",
    "from queue import Queue\n",
    "from threading import Thread\n",
    "\n",
    "\n",
    "class FeatureWorker(Thread):\n",
    "\n",
    "    def __init__(self, queue, csvwriter):\n",
    "        Thread.__init__(self)\n",
    "        self.queue = queue\n",
    "        self.csvwriter = csvwriter\n",
    "        self.verify_keys = False\n",
    "        self.first_keys = []\n",
    "        self.counter = 0\n",
    "\n",
    "    def run(self):\n",
    "        while True:\n",
    "            # Get the work from the queue and expand the tuple\n",
    "            features = self.queue.get()\n",
    "            \n",
    "            try:\n",
    "                self.counter = self.counter + 1\n",
    "                \n",
    "                if self.first_keys is None:\n",
    "                    keys=features.keys()\n",
    "\n",
    "                    self.first_keys = keys\n",
    "                    first_keys_str = ','.join(keys)\n",
    "\n",
    "                    print(f'Exporting {len(self.first_keys)} column features')\n",
    "\n",
    "                    csvwriter.writerow(self.first_keys)\n",
    "\n",
    "#                csvwriter.writerow(list(features.values()))\n",
    "                csvwriter.writerow(features)\n",
    "            finally:\n",
    "                self.queue.task_done()\n",
    "\n",
    "\n",
    "start = datetime.now()\n",
    "\n",
    "print(f'Starting at {start}')\n",
    "\n",
    "# Create a queue to communicate with the background thread\n",
    "queue = Queue()\n",
    "\n",
    "with open('test.csv', \"w\") as outfile:\n",
    "    csvwriter = csv.writer(outfile)\n",
    "    \n",
    "    worker = FeatureWorker(queue, csvwriter)\n",
    "\n",
    "    worker.daemon = True\n",
    "    worker.start()\n",
    "\n",
    "    # Comparable performance with using pool.imap directly, but the code is *much* cleaner\n",
    "    for features in pseq(row_df['values'], processes=5, partition_size=10)\\\n",
    "        .map(as_py_str)\\\n",
    "        .map(to_literal)\\\n",
    "        .map(randomise_sample)\\\n",
    "        .map(as_str_series)\\\n",
    "        .map(dropna)\\\n",
    "        .map(extract_features)\\\n",
    "        .map(values_only):\n",
    "            # enqueue returned features for background persistence\n",
    "            queue.put(features)\n",
    "\n",
    "    print('Waiting for background thread to complete')\n",
    "            \n",
    "    # wait for background tasks to complete\n",
    "    queue.join()\n",
    "    \n",
    "    print(f'Processed {worker.counter} items')\n",
    "\n",
    "print(f'Finished. Processed in {datetime.now() - start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
